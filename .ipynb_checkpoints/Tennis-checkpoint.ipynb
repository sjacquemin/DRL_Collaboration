{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> You will need to create the proper python virtual environment.  The environment specifications can be found in the requirements.txt file.\n",
    "<br>\n",
    "Using Anaconda is recommended.  To begin, execute the code below from your anaconda command prompt:<br>\n",
    "<br>\n",
    "$ conda env create -f environment.yaml\n",
    "<br>\n",
    "<br>\n",
    "Then navigate to the directory for this project where you downloaded it on your PC and launch jupyter notebook from there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from itertools import chain\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.setrecursionlimit(1500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> When finished, please leave the unity window open.  If you accidentally closed, shut down the kernel, relaunch the notebook and rerun the cells above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building our Actor and Critic Networks using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 'Actor' class builds an actor (policy) network that will map states to actions. Note the use of relu activation functions and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 128, fc2_units = 64):    \n",
    "        \n",
    "        # initializing model parameters\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.bn3 = nn.BatchNorm1d(action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        state = self.bn0(state)\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.bn3(F.relu(self.fc3(x)))\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 'Critic' class builds a critic (value) network that maps state,action pairs to Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units = 128, fc2_units = 64):    \n",
    "        \n",
    "        # initializing model parameters\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        state = self.bn0(state)\n",
    "        xs = self.fcs1(state)\n",
    "        xs = self.bn1(xs)\n",
    "        xs = F.leaky_relu(xs)\n",
    "        x = torch.cat((xs, action), dim = 1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from model architectures:</b> <br>\n",
    "<br>\n",
    "Model architectures were important in the successful training of the agents.  Two layer architectures worked best with 128 and 64 hidden units respectively and any deviation from these levels resulted in decreases in performance.  Both the actor and critic models needed extensive batch-normalization on almost every layer in order to learn properly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating a Noise Class to encourage exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise:\n",
    "    \n",
    "    def __init__(self, size, seed, mu = 0., theta = 0.15, sigma = 0.1, dt = 1):\n",
    "        \n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.dt = dt\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        # reset the noise to mean mu\n",
    "        \n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.array([np.random.randn() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from noise:</b> <br>\n",
    "<br>\n",
    "The initial DDPG implementation applied the random.random function instead of the np.random.randn function now used.  This was a problem given that random.random drew from a uniform distribution which significantly skewed the noise in one direction.  The np.random.rand function draws from a gaussian distribution which eliminates this skew.  Another learning was to keep the values of dt in the class to 1 (or exclude alltogether).  In other DDPG github implementations involving other environments, dt was used with a small value (~1e-2) which suppresses the noise.  While that may be helpful in other environments, it was detrimental here.  Finally, the value for sigma needed to be reduced to 0.1 from the 0.2 value used on the prior project.  This change effectively constrained the noise distribution by half.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Building a Replay Buffer for our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = self.buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                     field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # add a new experience to memory\n",
    "        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        # randomly sample a batch of experiences from memory\n",
    "        \n",
    "        experiences = random.sample(self.memory, k = self.batch_size) #self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from replay buffer designs:</b> <br>\n",
    "<br>\n",
    "The replay buffer utilized the same design as in the prior project.  Its two functions, add and sample, respectively add experience tuples to memory and randomly sample experience tuples from memory in order to aid in the learning process for the agents.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Initializing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1000000)                                              # replay buffer size\n",
    "BATCH_SIZE = 200                                                        # minibatch size\n",
    "GAMMA = 0.99                                                            # discount factor\n",
    "TAU = 0.001                                                             # for soft update of target parameters\n",
    "LR_ACTOR = 0.0012                                                       # learning rate of the actor\n",
    "LR_CRITIC = 0.001                                                       # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.00000                                                  # L2 weight decay (not used in final solution)\n",
    "LEARN_EVERY_T = 20                                                      # num time steps between each net update\n",
    "LR_DECAY = .000000                                                      # learning rate decay factor (not used in final solution)\n",
    "NB_AGENTS = 2                                                           # number of agents\n",
    "UPDATES_EACH_STEP = 10                                                  # number of updates to make every 'LEARN_EVERY_T' steps\n",
    "NOISE_DECAY = 0.000                                                     # amount noise is set to decay each step (not used)\n",
    "NOISE_FACTOR = 1.0                                                      # starting level of noise as % (not used in final solution)    \n",
    "NOISE_MIN = 0.1                                                         # minimum noise to keep (not used in final solution)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from hyperparameters:</b> <br>\n",
    "<br>\n",
    "<b>Batch size -</b> differing batch sizes did not result in significant changes in performance \n",
    "<br>\n",
    "<b>Gamma -</b>  The discount factor for future rewards.  Differing values of gamma between 0.99 and 1.0 did not change performance.  The objective is for the agents to keep the ball in the air as long as possible, so a high level of gamma needed to be maintained.\n",
    "<br>\n",
    "<b>TAU -</b>  Tau is used for the soft update of the target model parameters from the current model parameters.  Most implementations favor a value of 0.001 which was used in the end.  I ran an experiments in increasing the values for tau, but they resulted in significant declines in performance.\n",
    "<br>\n",
    "<b>Learning rates -</b>  While the ideal learning rate for the critic (0.001) was similar to those used in other DDPG implementations, the learning rate needed for the actor was much higher.  This was interesting given most DDPG implementations used a value of 0.0001.  The ideal actor learning rates for these agents was 12X higher at 0.0012.  This may be due to larger changes needing to be made in a shorter duration of time in order to capitalize on the rare observations where both agents are able to pass the ball to each other in the early stages of training.\n",
    "<br>\n",
    "<b>Weight decay -</b>  Used for regularization.  Weight decay did not help in training performance and therefore the value was set to zero.\n",
    "<br>\n",
    "<b>LR decay -</b>  To be used to dynamically decay learning rates over time.  This was not used in the final agent but I built in the option early on.\n",
    "<br>\n",
    "<b>LEARN_EVERY_T and UPDATES_EACH_STEP -</b> Here the agents only update their networks during every 'LEARN_EVERY_T' timesteps and they make 'UPDATES_EACH_STEP' updates when doing so.  These parameters effectively spread-out learning to balance improvements in performance with stability.  If the agents update too frequently, they can learn quickly but the solutions fail to stabilize and they can't maintain high performance for very long periods of time.\n",
    "<br>\n",
    "<b>NOISE_DECAY -</b>  To be used to dynamically decay noise over time.  This was not used in the final agent but I built in the option early on.\n",
    "<br>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Building the Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell creates one instance of an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, LR_ACTOR = LR_ACTOR, LR_CRITIC = LR_CRITIC, \n",
    "                 LR_DECAY = LR_DECAY, double_agent = 0):\n",
    "        \n",
    "        # initializing parameters\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.agent_score = []\n",
    "        self.lr_actor = LR_ACTOR\n",
    "        self.lr_critic = LR_CRITIC\n",
    "        self.lr_decay = LR_DECAY\n",
    "        self.nb_agents = NB_AGENTS\n",
    "        self.double_agent = double_agent\n",
    "        \n",
    "        # initializing models from step 4\n",
    "        \n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = LR_ACTOR)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = LR_CRITIC, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # initializing noise from step 5\n",
    "        \n",
    "        self.noise = Noise(action_size, random_seed)\n",
    "        \n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "    def act(self, state, noise_factor, add_noise = True):\n",
    "        \n",
    "        # returns actions for given state as per current policy\n",
    "        \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            first_noise = self.noise.sample()\n",
    "            if self.double_agent == 1:\n",
    "                action[0] += noise_factor * first_noise[0]\n",
    "                action[1] += noise_factor * first_noise[1]\n",
    "            else:\n",
    "                second_noise = self.noise.sample()\n",
    "                action[0][0] += noise_factor * first_noise[0]\n",
    "                action[0][1] += noise_factor * first_noise[1]\n",
    "                action[1][0] += noise_factor * second_noise[0]\n",
    "                action[1][1] += noise_factor * second_noise[1]\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        ###### update the critic ######\n",
    "        \n",
    "        # get predicted next state actions and Q values from target models\n",
    "\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # compute Q targets for current states\n",
    "        \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # compute critic loss\n",
    "\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        ###### update the actor ######\n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        ###### option to decay learning rates (not used in final solution) ######\n",
    "        \n",
    "        if self.lr_actor > 0.0001:\n",
    "            self.lr_actor -= self.lr_decay\n",
    "            self.lr_critic -= self.lr_decay\n",
    "\n",
    "        ###### update target networks ######\n",
    "        \n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \n",
    "        # soft update model parameters from local to target\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.detach_()\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)  \n",
    "            \n",
    "    def hard_update(self, target_model, local_model):\n",
    "        \n",
    "        for target_param, param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights about the learning algorithm:</b> <br>\n",
    "<br>\n",
    "Here I will be discussing the Agent() class. The agent proceeds along a repeating five stage process:\n",
    "<br>\n",
    "<br>\n",
    "1) The environment is reset and the initial states are passed to the agents\n",
    "<br>\n",
    "2) The agents execute their 'act' functions, which take the states and passes them through the actor networks referenced in section 4 of this notebook.  The actor networks pass the states through two hidden layers of 128 and 64 nodes respectively which are followed by relu activation and batch normalization functions (the critic shares a similar architecture but with activation functions coming before batch normalization steps).  The resulting output are floating-point values between -1 and 1 which represent the agents' next actions.  Output from the noise function referenced in section 5 are added to these values to encourage exploration.  Independent samples from the noise function also help the two seperate agents to, in effect, \"decouple\" and learn new actions without just mirroring each other.\n",
    "<br>\n",
    "3) The environment takes in the agents' action outputs and returns values corresponding to the next states, rewards and expressions (labeled 'dones') indicating whether or not the current episode is over.  The agents then take in tuples consisting of the initial states, actions of the agents, rewards, next states and dones and feed these experience tuples to the agents' shared replay buffer.\n",
    "<br>\n",
    "4) Every twenty time steps, the agents take a sample of their past experience tuples from their buffer and use the values in the tuples to update their actor and critic model parameters.  The agents accomplish this by executing their 'learn' functions, which update the loss values for both the actor and critic networks of the agent, which are in turn backpropogated through the network via ADAM optimizer functions which update the weights for both models.  This process repeats 10 times after every twenty time steps.\n",
    "<br>\n",
    "5) Finally, the values for the next states are set to be the values for the initial states and the algorithm repeats steps 2 through 5 until a given episode is complete and repeat steps 1 through 5 in subsequent episodes until the environment is considered solved.\n",
    "<br>\n",
    "<br>\n",
    "Detailed explanations for the hyperparameters used by the algorithm are presented in section 7 of the notebook above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\Anaconda3-1\\envs\\DRL\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling 100 Episodes Ending Ep 20\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 40\tAverage Score: 0.02                   Last 5 Episode Scores: 0.1, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 60\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 80\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 100\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 120\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.1, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 140\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 160\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 180\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.1, 0.1, 0.1, 0.0\n",
      "Rolling 100 Episodes Ending Ep 200\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 220\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 240\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 260\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 280\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 300\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 320\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 340\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 360\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 380\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 400\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 420\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 440\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 460\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 480\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.1, 0.2\n",
      "Rolling 100 Episodes Ending Ep 500\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 520\tAverage Score: 0.02                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.1, 0.0\n",
      "Rolling 100 Episodes Ending Ep 540\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 560\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 580\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 600\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 620\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 640\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 660\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 680\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.1, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 700\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.1, 0.0\n",
      "Rolling 100 Episodes Ending Ep 720\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 740\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 760\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.1, 0.0\n",
      "Rolling 100 Episodes Ending Ep 780\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 800\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 820\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 840\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 860\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 880\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 900\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 920\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 940\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 960\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 980\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1000\tAverage Score: 0.02                   Last 5 Episode Scores: 0.1, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1020\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1040\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.1, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1060\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1080\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1100\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1120\tAverage Score: 0.02                   Last 5 Episode Scores: 0.1, 0.0, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1140\tAverage Score: 0.03                   Last 5 Episode Scores: 0.0, 0.1, 0.0, 0.1, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1160\tAverage Score: 0.03                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1180\tAverage Score: 0.04                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1200\tAverage Score: 0.03                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1220\tAverage Score: 0.03                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1240\tAverage Score: 0.02                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1260\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1280\tAverage Score: 0.01                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1300\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1320\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1340\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1360\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1380\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling 100 Episodes Ending Ep 1400\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1420\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1440\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1460\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1480\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1500\tAverage Score: 0.00                   Last 5 Episode Scores: 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1520\tAverage Score: 0.01                   Last 5 Episode Scores: 0.0, 0.0, 0.1, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1540\tAverage Score: 0.02                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1560\tAverage Score: 0.04                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.2, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1580\tAverage Score: 0.05                   Last 5 Episode Scores: 0.2, 0.0, 0.1, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1600\tAverage Score: 0.06                   Last 5 Episode Scores: 0.1, 0.0, 0.1, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1620\tAverage Score: 0.07                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1640\tAverage Score: 0.08                   Last 5 Episode Scores: 0.1, 0.0, 0.2, 0.0, 0.2\n",
      "Rolling 100 Episodes Ending Ep 1660\tAverage Score: 0.08                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1680\tAverage Score: 0.10                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.2, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1700\tAverage Score: 0.10                   Last 5 Episode Scores: 0.1, 0.1, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1720\tAverage Score: 0.10                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.0, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1740\tAverage Score: 0.10                   Last 5 Episode Scores: 0.2, 0.0, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1760\tAverage Score: 0.10                   Last 5 Episode Scores: 0.1, 0.3, 0.1, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1780\tAverage Score: 0.10                   Last 5 Episode Scores: 0.2, 0.1, 0.1, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1800\tAverage Score: 0.11                   Last 5 Episode Scores: 0.1, 0.0, 0.0, 0.1, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1820\tAverage Score: 0.15                   Last 5 Episode Scores: 0.1, 0.1, 0.4, 0.9, 0.1\n",
      "Rolling 100 Episodes Ending Ep 1840\tAverage Score: 0.24                   Last 5 Episode Scores: 0.6, 0.4, 1.0, 1.0, 0.6\n",
      "Rolling 100 Episodes Ending Ep 1860\tAverage Score: 0.23                   Last 5 Episode Scores: 0.1, 0.6, 0.0, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1880\tAverage Score: 0.24                   Last 5 Episode Scores: 0.1, 0.0, 0.1, 0.0, 1.4\n",
      "Rolling 100 Episodes Ending Ep 1900\tAverage Score: 0.26                   Last 5 Episode Scores: 0.0, 0.2, 0.2, 0.0, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1920\tAverage Score: 0.31                   Last 5 Episode Scores: 0.4, 1.2, 0.2, 1.3, 2.6\n",
      "Rolling 100 Episodes Ending Ep 1940\tAverage Score: 0.32                   Last 5 Episode Scores: 0.4, 0.1, 0.1, 0.7, 0.0\n",
      "Rolling 100 Episodes Ending Ep 1960\tAverage Score: 0.42                   Last 5 Episode Scores: 0.3, 0.1, 0.3, 0.2, 0.3\n",
      "Environment Solved in  1975  episodes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "num_agents = 2\n",
    "agents = Agent(state_size, action_size, random_seed = 42, prio = False)\n",
    "\n",
    "def ddpg_multiple(n_episodes = 10000, max_t = 10000, print_every = 20):  \n",
    "    \n",
    "    scores_deque = deque(maxlen = 100)\n",
    "    scores = []  \n",
    "    env_info = env.reset(train_mode=True)[brain_name] \n",
    "    t_step = 0\n",
    "    memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed = 1)\n",
    "        # this creates the replay buffer to be shared between the two agents\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        episode_scores = np.zeros(num_agents)                                # initialize/reset the episode scores\n",
    "        env_info = env.reset()[brain_name]                                   # reset the environment   \n",
    "        states = env_info.vector_observations                                # get the current state (for each agent)\n",
    "                                                             \n",
    "        for t in range(max_t):\n",
    "\n",
    "            actions = agents.act(states, noise_factor = NOISE_FACTOR)        # agent selects its action\n",
    "            env_info = env.step(actions)[brain_name]                         # the action is passed to the environment\n",
    "            next_states = env_info.vector_observations                       # get next states \n",
    "            rewards = env_info.rewards                                       # get rewards\n",
    "            dones = env_info.local_done                                      # see if episode finished\n",
    "            episode_scores += env_info.rewards                               # update the agent scores       \n",
    "            memory.add(states[0], actions[0], rewards[0], next_states[0], dones[0])  # add each experience tuple to the buffer\n",
    "            memory.add(states[1], actions[1], rewards[1], next_states[1], dones[1])\n",
    "            t_step = (t_step + 1) % LEARN_EVERY_T\n",
    "            if t_step  == 0:                                                 # update model parameters every 'LEARN_EVERY_T'\n",
    "                if len(memory)>BATCH_SIZE:                                           # timesteps\n",
    "                    for j in range(UPDATES_EACH_STEP):\n",
    "                        experiences = memory.sample()\n",
    "                        agents.learn(experiences, GAMMA)             \n",
    "            states = next_states                                             # next states becomes the current states\n",
    "            if np.any(dones):                                                # exit loop if episode finished\n",
    "                break       \n",
    "\n",
    "        scores_deque.append(np.max(episode_scores))\n",
    "        scores.append(np.max(episode_scores))\n",
    " \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rRolling 100 Episodes Ending Ep {}\\tAverage Score: {:.2f}   \\\n",
    "                Last 5 Episode Scores: {:.1f}, {:.1f}, {:.1f}, {:.1f}, {:.1f}'.format(\n",
    "                i_episode, np.mean(scores_deque), scores_deque[-5], scores_deque[-4], scores_deque[-3], \n",
    "                scores_deque[-2], scores_deque[-1]))\n",
    "\n",
    "        if np.mean(scores_deque) >= 0.5:                                      # save models and break if environment solved\n",
    "            print('Environment Solved in ', i_episode, ' episodes.')\n",
    "            torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg_multiple()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from training:</b> <br>\n",
    "<br>\n",
    "As seen in the above line chart, the agents successfully solve the environment after 1,975 episodes.  The most important learnings were stated in section 7 as it pertains to hyperparameter tuning.  In addition to these, there were a couple of interesting phenomena that I observed.  First, it seems that simple networks (in terms of number of nodes per hidden layer) were more effective with higher learning rates, but several of my earlier attempts involved more complex networks with smaller learning rates and these attempts, while not ultimately successful, did perform better than combining complex networks with large learning rates and combining simple networks with smaller learning rates.  It's interesting that there seems to be an inverse relationship between network complexity and learning rates in terms of successful performance.  The second interesting phenomenon was that there were several attempts where my rolling average scores had higher performance earlier on, with long strings of 0.04 and/or 0.05 but ultimately did not achieve the \"lift off\" that the final solution did even though the final solution had respective rolling averages of around 0.01 beforehand.  I believe these earlier attempts resulted in the agents learning how to hit the ball one time really well, but not learning how to pass the ball back and forth and perform cooperatively.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  See the trained smart agents in action!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Run the below cell to see the intelligent agents in action.  The agents are trained to keep the ball in the air for as long as possible.\n",
    "<br>\n",
    "<br>\n",
    "If your unity window closed from earlier, please shut down the kernel, relaunch the notebook and rerun all cells except the training cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\Anaconda3-1\\envs\\DRL\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agents = Agent(state_size, action_size, random_seed = 42)\n",
    "agents.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agents.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "env_info = env.reset(train_mode=False)[brain_name]                    \n",
    "states = env_info.vector_observations     \n",
    "#state_size = states.shape[1]\n",
    "while True:\n",
    "    actions = agents.act(states, noise_factor = NOISE_FACTOR)     \n",
    "    env_info = env.step(actions)[brain_name]         \n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards   \n",
    "    dones = env_info.local_done\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Ideas for improvement:</b> <br>\n",
    "<br>\n",
    "This solution utilizes a DDPG framework.  PPO or other actor-critic frameworks could be applied and could potentially result in an improved solution.  Another idea is to implement learning rate decay, starting training with large learning rates which gradually decline as the agents train and eventually stablize with very low learning rate values.  A noise decay parameter could be used as well along the same lines of logic.  Prioritized replay, Double Q networks or other Q-network improvements could also be applied to the critic networks which could potentially improve performance.\n",
    "<br>\n",
    "<br>\n",
    "One radical change could involve feeding the output of convolutional neural networks in favor of environment states to the actor and critic networks.  This would behave similarly to prior Q-network achievements on Atari games seen in the literature where the agents learn from variations in screen pixels.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Appendix / Code Not Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I coded the below cells in prior attempts to find a solution.  In the end, these cells weren't necessary to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    \n",
    "    def __init__(self, gamma = GAMMA, tau = TAU, noise_decay = NOISE_DECAY, noise_factor = NOISE_FACTOR, noise_min = NOISE_MIN):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.iter = 0\n",
    "        self.noise_decay = noise_decay\n",
    "        self.noise_factor = noise_factor\n",
    "        self.noise_min = noise_min\n",
    "        \n",
    "        self.maddpg_agent = [ Agent(state_size, action_size, random_seed = 1, prio = False, double_agent = 1),\n",
    "                             Agent(state_size, action_size, random_seed = 1, prio = False, double_agent = 1) ]\n",
    "        \n",
    "    def get_actors(self):\n",
    "        \n",
    "        # retrieve actor networks from individual agents in maddp_agent\n",
    "        \n",
    "        local_actors = [agent.actor_local for agent in self.maddpg_agent]\n",
    "        return local_actors\n",
    "        \n",
    "    def get_target_actors(self):\n",
    "        \n",
    "        # retrieve the target actor networks from individual agents in maddp_agent\n",
    "        \n",
    "        target_actors = [agent.actor_target for agent in self.maddpg_agent]\n",
    "        return target_actors\n",
    "        \n",
    "    def act(self, states):\n",
    "        \n",
    "        # gets actions from all agents in maddpg_agent\n",
    "        \n",
    "        actions = [agent.act(state, self.noise_factor) for agent, state in zip(self.maddpg_agent, states)]\n",
    "        return actions\n",
    "    \n",
    "    def target_act(self, next_states):\n",
    "        \n",
    "        # get target network actions from all agents in maddpg_agent\n",
    "        \n",
    "        target_actions = [agent.actor_target(next_state) for agent, next_state in zip(self.maddpg_agent, next_states)]\n",
    "        return target_actions\n",
    "    \n",
    "    def update(self, experiences, agent_number, logger):\n",
    "        \n",
    "        # updates the critics and actors of all the agents\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "\n",
    "        target_actions = agent.actor_target(next_states)\n",
    "        Q_targets_next = agent.critic_target(next_states, target_actions)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # compute the critic loss\n",
    "        \n",
    "        Q_expected = agent.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        agent.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(agent.critic_local.parameters(), 1)\n",
    "        agent.critic_optimizer.step()\n",
    "        \n",
    "        ###### update the actor ######\n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        #actions_pred = [ self.maddpg_agent[i].actor_local(state) if i == agent_number else\n",
    "        #               self.maddpg_agent[i].actor_local(state).detach() for i, state in enumerate(states)]\n",
    "        actions_pred = agent.actor_local(states)\n",
    "        actor_loss = -agent.critic_local(states, actions_pred).mean()\n",
    "        actor_loss = Variable(actor_loss, requires_grad = True)\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        agent.actor_optimizer.step()\n",
    "        \n",
    "        ###### obtain info for torchboard ######\n",
    "        \n",
    "        al = actor_loss.cpu().detach().item()\n",
    "        cl = critic_loss.cpu().detach().item()\n",
    "        logger.add_scalar('critic_loss', cl, self.iter)\n",
    "        logger.add_scalar('actor_loss', al, self.iter)\n",
    "        \n",
    "        ###### decay noise ######\n",
    "        \n",
    "        if (self.noise_factor - self.noise_decay) > self.noise_min:\n",
    "            self.noise_factor -= self.noise_decay\n",
    "        else:\n",
    "            self.noise_factor = self.noise_min\n",
    "            \n",
    "        ###### option to decay learning rates ######\n",
    "        agent.lr_actor *= agent.lr_decay\n",
    "        agent.lr_critic *= agent.lr_decay\n",
    "        \n",
    "    def update_targets(self):\n",
    "        \n",
    "        # soft update of targets\n",
    "        \n",
    "        self.iter += 1\n",
    "        for agent in self.maddpg_agent:\n",
    "            agent.soft_update(agent.actor_target, agent.actor_local, self.tau)\n",
    "            agent.soft_update(agent.critic_target, agent.critic_local, self.tau)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
